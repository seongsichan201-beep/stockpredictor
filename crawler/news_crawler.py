from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time
import json
import datetime
import os

try:
    from webdriver_manager.chrome import ChromeDriverManager
    _USE_WEBDRIVER_MANAGER = True
except Exception:
    _USE_WEBDRIVER_MANAGER = False


def run_auto_crawler():
    print("=== Google ë‰´ìŠ¤ ìë™ í¬ë¡¤ë§ ì‹œì‘ ===")

    options = Options()
    options.add_argument("--headless=new")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("--disable-infobars")
    options.add_argument("--disable-extensions")
    options.add_argument("--disable-browser-side-navigation")

    if _USE_WEBDRIVER_MANAGER:
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=options)
    else:
        driver = webdriver.Chrome(options=options)

    # "í˜ì´ì§€ ë¡œë”© ì‹œê°„ ì œí•œ" ê°•ì œ ì ìš©
    driver.set_page_load_timeout(8)

    url = "https://news.google.com/home?hl=ko&gl=KR&ceid=KR%3Ako"
    driver.get(url)
    time.sleep(2)

    print("ìŠ¤í¬ë¡¤ ì¤‘...")

    last_height = driver.execute_script("return document.body.scrollHeight")
    for _ in range(8):
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(1.2)
        new_height = driver.execute_script("return document.body.scrollHeight")
        if new_height == last_height:
            break
        last_height = new_height

    print("ë§í¬ ìˆ˜ì§‘ ì¤‘...")

    selectors = ["a.DY5T1d", "a.JtKRv", "a.WwrzSb"]
    links = []

    for selector in selectors:
        elems = driver.find_elements(By.CSS_SELECTOR, selector)
        for e in elems:
            url = e.get_attribute("href")
            if url and url not in links:
                links.append(url)

    print(f"ì´ {len(links)}ê°œ ë§í¬ ê°ì§€")

    news_data = []

    # ê¸°ì‚¬ ë°˜ë³µ ìŠ¤ìº”
    for idx, link in enumerate(links, start=1):
        print(f"\nâ–¶ {idx}ë²ˆì§¸ ê¸°ì‚¬ ìŠ¤ìº” ì¤‘â€¦")

        try:
            driver.get(link)
        except Exception:
            print("âš  í˜ì´ì§€ ë¡œë”© ì‹¤íŒ¨ â†’ ê±´ë„ˆëœ€")
            continue

        time.sleep(1)

        # ì œëª© 3ì´ˆ íƒ€ì„ì•„ì›ƒ
        try:
            title_elem = WebDriverWait(driver, 3).until(
                EC.presence_of_element_located((By.TAG_NAME, "h1"))
            )
            title = title_elem.text.strip()
        except Exception:
            title = "ì œëª© ì—†ìŒ"
            print("âš  ì œëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")

        # ë³¸ë¬¸ 3ì´ˆ íƒ€ì„ì•„ì›ƒ
        try:
            paragraphs = WebDriverWait(driver, 3).until(
                EC.presence_of_all_elements_located((By.TAG_NAME, "p"))
            )
            body = "\n".join([p.text.strip() for p in paragraphs if p.text.strip()])
        except Exception:
            body = ""
            print("âš  ë³¸ë¬¸ ì—†ìŒ")

        # ì œëª© + ë³¸ë¬¸ ë‘˜ ë‹¤ ì—†ìŒ â†’ PASS
        if title == "ì œëª© ì—†ìŒ" and body == "":
            print("âš  ê¸°ì‚¬ ë‚´ìš© ì—†ìŒ â†’ ë‹¤ìŒìœ¼ë¡œ")
            continue

        news_data.append({
            "url": link,
            "title": title,
            "body": body
        })

        print(f"âœ” ìŠ¤ìº” ì„±ê³µ: {title}")

    driver.quit()

    # ì €ì¥
    os.makedirs("news_daily", exist_ok=True)
    today = datetime.datetime.now().strftime("%Y-%m-%d")
    filename = f"news_daily/news_{today}.json"

    with open(filename, "w", encoding="utf-8") as f:
        json.dump(news_data, f, ensure_ascii=False, indent=4)

    print(f"\nğŸ“ ì €ì¥ ì™„ë£Œ â†’ {filename}")
    print("=== í¬ë¡¤ë§ ì¢…ë£Œ ===")
    

if __name__ == "__main__":
    run_auto_crawler()
